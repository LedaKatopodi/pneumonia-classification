{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Classification\n",
    "\n",
    "## Neural Network Approach\n",
    "\n",
    "The purpose of this approach is dual; to build a neural network that can accurately predict pneumonia from X-ray images, and explore the various image processing techniques that can lead to more robust models.\n",
    "\n",
    "A simple Convolutional Neural Network was constructed, utilizing tensorflow and keras. Transfer learning was also implemented for the initialization of the first layers of the neural network, taking the weights from a neural network model trained on Imagenet. Use of weights from a pre-trained model can provide critical advantages for both the performance of a neural network and its accuracy; essentially the first models capture general details and fine-tuning is a much better approach than randomly initializing them.\n",
    "\n",
    "The classes that we will work with for this approach are normal/pneumonia, and we will not delve into the sub-classes of the pneumonia class (viral/bacterial).\n",
    "\n",
    "### Setup\n",
    "\n",
    "#### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Input\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras.layers import GlobalMaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "from keras.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "import tensorflow as tf\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import glob\n",
    "import h5py\n",
    "import shutil\n",
    "import imgaug as aug\n",
    "import imgaug.augmenters as iaa\n",
    "\n",
    "import os\n",
    "from os import listdir, makedirs, getcwd, remove\n",
    "from os.path import isfile, join, abspath, exists, isdir, expanduser\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Auxiliary Functions\n",
    "\n",
    "The following auxiliary function **adjust_gamma** is utilized during the pre-processing of the input images, and eprforms gamma correction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_gamma(image, gamma=1.0):\n",
    "    #lookup table with the adjusted gamma values\n",
    "    invGamma = 1.0 / gamma\n",
    "    table = np.array([((i / 255.0) ** invGamma) * 255\n",
    "        for i in np.arange(0, 256)]).astype(\"uint8\")\n",
    "\n",
    "    # apply gamma correction using the lookup table\n",
    "    return cv2.LUT(image, table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to the data directory\n",
    "data_dir = Path('/content/drive/MyDrive/Colab Notebooks/ImageProcessing/chest_xray')\n",
    "\n",
    "# Path to train directory\n",
    "train_dir = data_dir / 'train'\n",
    "\n",
    "# Path to validation directory\n",
    "val_dir = data_dir / 'val'\n",
    "\n",
    "# Path to test directory\n",
    "test_dir = data_dir / 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the normal and pneumonia sub-directories\n",
    "normal_cases_dir = train_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
    "\n",
    "# Get the list of all the images\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "train_df = []\n",
    "\n",
    "# Normal cases, setting label = 0\n",
    "for img in normal_cases:\n",
    "    train_df.append((img,0))\n",
    "\n",
    "# Pneumonia cases, setting label = 1\n",
    "for img in pneumonia_cases:\n",
    "    train_df.append((img, 1))\n",
    "\n",
    "\n",
    "train_df = pd.DataFrame(train_df, columns=['image', 'label'],index=None) # data frame\n",
    "train_df = train_df.sample(frac=1.).reset_index(drop=True) # shuffling\n",
    "train_df.head() # inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we further inspect the training data, to check whether the two classes (normal/pneumonia) are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Inspection\n",
    "# Get the counts for each class\n",
    "cases_count = train_df['label'].value_counts()\n",
    "print(cases_count)\n",
    "normal_counts = cases_count[0]\n",
    "pneumonia_counts = cases_count[1]\n",
    "\n",
    "# Plot the results \n",
    "plt.figure(figsize=(8,10))\n",
    "sns.set_palette(\"Set2\", 10)\n",
    "sns.barplot(x=cases_count.index, y= cases_count.values)\n",
    "plt.title('Number of cases', fontsize=14)\n",
    "plt.xlabel('Case type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)'])\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Pneumonia:Normal = \" + str(pneumonia_counts/normal_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the barchart above, the two classes are **not balanced**. This is definitely an issue for the model, particularly during the Training step. \n",
    "\n",
    "To this end, we are going to apply data augmentation, and try to generate extra instances of images that belong to the normal class starting from images already present in our dataset. As shown above the ratio of normal to pneumonia images is approximately 3, meaning that for every image that belongs to the normal class we have 3 images that belong to the pneumonia class, in the training set.\n",
    "\n",
    "For this reason, we are going to generate 2 additional images per already existing normal cases by applying a random data augmentation technique, taken from a set of potential techniques. The new normal case images are saved along with the existing ones in a new folder, creating a new \"augmented\"/\"extended\" set of training images.\n",
    "\n",
    "The process described above only takes place to only augment the training set. For the validation and test sets we will be working with the existing ones for two main reasons: firstly, it is more important to *train* the model with a balanced set that validate or test it, and, secondly, in the majority of real-life scenarios the test or validation sets will not be balanced. With this mindset, we will continue with augmenting and splitting our dataset.\n",
    "\n",
    "##### Data Augmentation\n",
    "\n",
    "From the possible augmentation techniques we choose the following:\n",
    "\n",
    "* Horizontal flip\n",
    "* Rotation: either at a 20° angle or at a -15° one.\n",
    "* Brightness increase: multiplying the pixel value by either 1.1 or 1.5\n",
    "* Sharpening\n",
    "\n",
    "An auxiliary function **enrich_normal** to perform data augmentation is also defined here. Its arguments are:\n",
    "\n",
    "* *normal_train_df*: the dataframe containing the paths to the normal dataset instances.\n",
    "* *outdir*: the output directory to write the newly generated instances of normal cases\n",
    "* *extra*: the number of additional images to generate for each input normal image. In our case, since the ratio Pneumonia:Normal is close to 3, we need additional 2 images per normal image to be generated so that we achieve a more balanced training set.\n",
    "\n",
    "Running **enrich_normal** will first resize the input image to (224,224) and then perform the augmentation, by choosing one of the techniques above, at random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmentation sequence \n",
    "seq = iaa.OneOf([\n",
    "    iaa.Fliplr(), # horizontal flips\n",
    "    iaa.Affine(rotate=20), # rotation\n",
    "    iaa.Affine(rotate=-15), # rotation\n",
    "    iaa.Multiply((1.1, 1.5)), #random brightness\n",
    "    iaa.Sharpen(alpha=0.5)]) # sharpening\n",
    "\n",
    "# Auxiliary function for data generation\n",
    "def enrich_normal(normal_train_df, outdir, extra):\n",
    "    os.chdir(outdir)\n",
    "\n",
    "    # Get total number of samples in the data\n",
    "    n = len(normal_train_df)\n",
    "    \n",
    "    count = 1\n",
    "    for i in range(n):\n",
    "        img_name = normal_train_df.iloc[i]['image']\n",
    "      \n",
    "        # read the image and resize\n",
    "        img = cv2.imread(str(img_name))\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        orig_img = img.copy()\n",
    "        name = \"NORMAL_EXTENDED_\" + str(count) + \".jpeg\"\n",
    "        cv2.imwrite(name, orig_img)\n",
    "        count +=1\n",
    "\n",
    "        # Generating additional normal cases\n",
    "        for j in range(extra):\n",
    "            \n",
    "            aug_img = seq.augment_image(img)\n",
    "            name = \"NORMAL_EXTENDED_\" + str(count) + \".jpeg\"\n",
    "            cv2.imwrite(name, aug_img)\n",
    "            count +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below generates and saves the additional normal instances. Running this is quite a consuming step, thus the code has been commened out and the final training set is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sub-setting the normal cases\n",
    "# normal_train_df = train_df[train_df['label'] == 0] \n",
    "# outdir = train_dir / 'NORMAL_EXTENDED'\n",
    "# # os.mkdir(outdir) # run once\n",
    "\n",
    "# # Generate Extended normal images\n",
    "# enrich_normal(normal_train_df, outdir, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New Training data\n",
    "\n",
    "# Get the path to the normal and pneumonia sub-directories\n",
    "normal_cases_dir = train_dir / 'NORMAL_EXTENDED'\n",
    "pneumonia_cases_dir = train_dir / 'PNEUMONIA'\n",
    "\n",
    "# Get the list of all the images\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "new_train_df = []\n",
    "\n",
    "# Normal cases, setting label = 0\n",
    "for img in normal_cases:\n",
    "    new_train_df.append((img,0))\n",
    "\n",
    "# Pneumonia cases, setting label = 1\n",
    "for img in pneumonia_cases:\n",
    "    new_train_df.append((img, 1))\n",
    "\n",
    "new_train_df = pd.DataFrame(new_train_df, columns=['image', 'label'],index=None) # dataframe\n",
    "new_train_df = new_train_df.sample(frac=1.).reset_index(drop=True) # shuffling\n",
    "new_train_df.head() # inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual Inspection\n",
    "# Get the counts for each class\n",
    "cases_count = new_train_df['label'].value_counts()\n",
    "print(cases_count)\n",
    "\n",
    "# Plot the results \n",
    "plt.figure(figsize=(8,10))\n",
    "sns.set_palette(\"Set2\", 10)\n",
    "sns.barplot(x=cases_count.index, y= cases_count.values)\n",
    "plt.title('Number of cases', fontsize=14)\n",
    "plt.xlabel('Case type', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, a more balanced training set is now available.\n",
    "\n",
    "#### Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the sub-directories\n",
    "normal_cases_dir = val_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = val_dir / 'PNEUMONIA'\n",
    "\n",
    "# Get the list of all the images\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "valid_df = []\n",
    "\n",
    "# Normal cases, setting label = 0\n",
    "for img in normal_cases:\n",
    "    valid_df.append((img,0))\n",
    "\n",
    "# Pneumonia cases, setting label = 1\n",
    "for img in pneumonia_cases:\n",
    "    valid_df.append((img, 1))\n",
    "\n",
    "valid_df = pd.DataFrame(valid_df, columns=['image', 'label'],index=None) # dataframe\n",
    "valid_df = valid_df.sample(frac=1.).reset_index(drop=True) # shuffling\n",
    "valid_df.head() # inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to the sub-directories\n",
    "normal_cases_dir = test_dir / 'NORMAL'\n",
    "pneumonia_cases_dir = test_dir / 'PNEUMONIA'\n",
    "\n",
    "# Get the list of all the images\n",
    "normal_cases = normal_cases_dir.glob('*.jpeg')\n",
    "pneumonia_cases = pneumonia_cases_dir.glob('*.jpeg')\n",
    "\n",
    "test_df = []\n",
    "\n",
    "# Normal cases, setting label = 0\n",
    "for img in normal_cases:\n",
    "    test_df.append((img,0))\n",
    "\n",
    "# Pneumonia cases, setting label = 1\n",
    "for img in pneumonia_cases:\n",
    "    test_df.append((img, 1))\n",
    "\n",
    "test_df = pd.DataFrame(test_df, columns=['image', 'label'],index=None) # dataframe\n",
    "test_df = test_df.sample(frac=1.).reset_index(drop=True) # shuffling\n",
    "test_df.head() # inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing\n",
    "\n",
    "It is essential that we pre-process the input images -belonging either to the training, validation, or test set- so that they have the appropriate \"properties\" before supplying them to the neural network. We introduce a \"base pre-processing\" approach that includes all the necessary steps prior to supplying an image as input to the model, as well as some additional pre-processing techniques that we believe will enhance the images and provide a better accuracy for the model.\n",
    "\n",
    "#### Base Pre-Processing\n",
    "\n",
    "With base pre-processing, we prform the following steps:\n",
    "\n",
    "1. Image resizing: The model accepts a (112,112) image as input and, therefore, all images are resized to (112,112).\n",
    "2. Channel Correction: Some images, even if they appear to be in grayscale, are in fact RGB images. We create the RGB versions of the images for all of the input images by stacking the color channel thrice, if the image is in grayscale.\n",
    "3. Grayscale: Now that all images are in RGB, we can transform them all into grayscale so that we can supply them into the model.\n",
    "4. Normalization: The pixel values are normalized by dividing their values with 255.\n",
    "\n",
    "These steps are all essential in order to provide appropriate input for the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_pre(data):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "\n",
    "    normal_images = data.loc[data['label'] == 0,'image']\n",
    "    pneumonia_images = data.loc[data['label'] == 1,'image']\n",
    "\n",
    "    for img_name in normal_images:\n",
    "        img = cv2.imread(str(img_name))\n",
    "        # Resizing\n",
    "        img = cv2.resize(img, (112,112))\n",
    "        # Channel correction\n",
    "        if img.shape[2] ==1:\n",
    "            img = np.dstack([img, img, img])\n",
    "        # Grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Normalization\n",
    "        img = img.astype(np.float32)/255.\n",
    "        label = to_categorical(0, num_classes=2)\n",
    "        new_data.append(img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "\n",
    "    for img_name in pneumonia_images:\n",
    "        img = cv2.imread(str(img_name))\n",
    "        # Resizing\n",
    "        img = cv2.resize(img, (112,112))\n",
    "        # Channel correction\n",
    "        if img.shape[2] == 1:\n",
    "          img = np.dstack([img, img, img])\n",
    "        # Grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Normalization\n",
    "        img = img.astype(np.float32)/255.\n",
    "        label = to_categorical(1, num_classes=2)\n",
    "        new_data.append(img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    # Convert the list into numpy arrays\n",
    "    new_data = np.array(new_data)\n",
    "    new_labels = np.array(new_labels)\n",
    "\n",
    "    return new_data, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process the data utilizing the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, train_labels = base_pre(new_train_df)\n",
    "valid_data, valid_labels = base_pre(valid_df)\n",
    "test_data, test_labels = base_pre(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from the essential pre-processign steps, we are also looking into further pre-processing steps that may (or may not) enhance the image quality and provide more accurate results when running the neural network.\n",
    "\n",
    "#### Simple Pre-processing\n",
    "\n",
    "Apart from image resizing, channel correction, grayscale transformation, and normalization we also create an alternative preprocessing approach, by introducing additional image pre-processing steps:\n",
    "\n",
    "* Negative filter\n",
    "* Addition\n",
    "\n",
    "The above (simple) image pre-processing techniques are coupled with the base pre-processing ones and included in the following auxiliary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_simple(data):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "\n",
    "    normal_images = data.loc[data['label'] == 0,'image']\n",
    "    pneumonia_images = data.loc[data['label'] == 1,'image']\n",
    "\n",
    "    for img in normal_images:\n",
    "        img = cv2.imread(str(img))\n",
    "\n",
    "        # Resizing\n",
    "        img = cv2.resize(img, (112,112))\n",
    "        # Channel correction\n",
    "        if img.shape[2] ==1:\n",
    "            img = np.dstack([img, img, img])\n",
    "        # Grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # == Extra Preprocessing == #\n",
    "        img = (255-img)\n",
    "        img = cv2.add(img, -30)\n",
    "        # == #\n",
    "\n",
    "        # Normalization\n",
    "        img = img.astype(np.float32)/255.\n",
    "        label = to_categorical(0, num_classes=2)\n",
    "        new_data.append(img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "\n",
    "    for img in pneumonia_images:\n",
    "        img = cv2.imread(str(img))\n",
    "\n",
    "        # Resizing\n",
    "        img = cv2.resize(img, (112,112))\n",
    "        # Channel correction\n",
    "        if img.shape[2] ==1:\n",
    "            img = np.dstack([img, img, img])\n",
    "        # Grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # == Extra Preprocessing == #\n",
    "        img = (255-img)\n",
    "        img = cv2.add(img, -30)\n",
    "        # == #\n",
    "\n",
    "        # Normalization\n",
    "        img = img.astype(np.float32)/255.\n",
    "        label = to_categorical(1, num_classes=2)\n",
    "        new_data.append(img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    # Convert the list into numpy arrays\n",
    "    new_data = np.array(new_data)\n",
    "    new_labels = np.array(new_labels)\n",
    "\n",
    "    return new_data, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process the data utilizing the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2, train_labels_2 = pre_simple(new_train_df)\n",
    "valid_data_2, valid_labels_2 = pre_simple(valid_df)\n",
    "test_data_2, test_labels_2 = pre_simple(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histogram Normalization\n",
    "\n",
    "We further introduce histogram normalization to the pre-processing steps, and define the appropriate auxiliary function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_histo(data):\n",
    "    new_data = []\n",
    "    new_labels = []\n",
    "\n",
    "    normal_images = data.loc[data['label'] == 0, 'image']\n",
    "    pneumonia_images = data.loc[data['label'] == 1, 'image']\n",
    "\n",
    "    for img in normal_images:\n",
    "        img = cv2.imread(str(img))\n",
    "\n",
    "        # == Extra Preprocessing == #\n",
    "        hist, bins = np.histogram(img.flatten(), 256, [0, 256])\n",
    "        cdf = hist.cumsum()\n",
    "        cdf_normalized = cdf * hist.max() / cdf.max()\n",
    "        cdf_m = np.ma.masked_equal(cdf, 0)\n",
    "        cdf_m = (cdf_m - cdf_m.min()) * 255 / (cdf_m.max() - cdf_m.min())\n",
    "        cdf2 = np.ma.filled(cdf_m, 0).astype('uint8')\n",
    "\n",
    "        img = cdf2[img]\n",
    "        img = (255 - img)\n",
    "        img = cv2.add(img, -70)\n",
    "        # == #\n",
    "\n",
    "        # Resizing\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "        # Channel correction\n",
    "        if img.shape[2] == 1:\n",
    "            img = np.dstack([img, img, img])\n",
    "        # Grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        # Normalization\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        label = to_categorical(0, num_classes=2)\n",
    "        new_data.append(img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    for img in pneumonia_images:\n",
    "        img = cv2.imread(str(img))\n",
    "        # Resizing\n",
    "        img = cv2.resize(img, (112, 112))\n",
    "        # Channel correction\n",
    "        if img.shape[2] == 1:\n",
    "            img = np.dstack([img, img, img])\n",
    "        # Grayscale\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # == Extra Preprocessing == #\n",
    "        hist, bins = np.histogram(img.flatten(), 256, [0, 256])\n",
    "        cdf = hist.cumsum()\n",
    "        cdf_normalized = cdf * hist.max() / cdf.max()\n",
    "        cdf_m = np.ma.masked_equal(cdf, 0)\n",
    "        cdf_m = (cdf_m - cdf_m.min()) * 255 / (cdf_m.max() - cdf_m.min())\n",
    "        cdf2 = np.ma.filled(cdf_m, 0).astype('uint8')\n",
    "\n",
    "        img = cdf2[img]\n",
    "        img = (255 - img)\n",
    "        img = cv2.add(img, -70)\n",
    "        # == #\n",
    "\n",
    "        # Normalization\n",
    "        img = img.astype(np.float32) / 255.\n",
    "        label = to_categorical(1, num_classes=2)\n",
    "        new_data.append(img)\n",
    "        new_labels.append(label)\n",
    "\n",
    "    # Convert the list into numpy arrays\n",
    "    new_data = np.array(new_data)\n",
    "    new_labels = np.array(new_labels)\n",
    "\n",
    "    return new_data, new_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pre-process the data utilizing the function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_4, train_labels_4 = pre_histo(new_train_df)\n",
    "valid_data_4, valid_labels_4 = pre_histo(valid_df)\n",
    "test_data_4, test_labels_4 = pre_histo(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also note that we tested two different *Edge Detection* filters, the Sobel and Canny filters in order to see whether the processed images would \"help\" the model provide more accurate results. Unfortunately, running the model led to an accuracy of 50% during the validation step, which practically means the model assigned labels at random, and the *Edge Detection* approach as a pre-processing step for modifying the NN input was discarded.\n",
    "\n",
    "Below we inspect some example images from the 3 pre-processed sets we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "image = train_data[0]\n",
    "plt.subplot(1, 3, 1), plt.imshow(image, cmap='gray'), plt.axis(False)\n",
    "image = train_data_2[0]\n",
    "plt.subplot(1, 3, 2), plt.imshow(image, cmap='gray'), plt.axis(False)\n",
    "image = train_data_4[0]\n",
    "plt.subplot(1, 3, 3), plt.imshow(image, cmap='gray'), plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model: Construction & Testing\n",
    "\n",
    "For the purposes of classifying the normal/pneumonia cases a very simple neural model is constructed. Its input size is (112,112,1) and, therefore, all images have already been pre-processed to this size. Its structure is very simple, including 4 convolutional layers divided in 2 pairs, with MaxPooling and Dropout layers between each pair. Transfer learning was also utilized in the frame of this project by introducing weights to the first two layers from a model pre-trained on imagenet.\n",
    "\n",
    "Below we define an auxiliary function that builds the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    inp = Input(shape=(112, 112, 1), name='ImageInput')\n",
    "\n",
    "    hid = Conv2D(32, (3, 3),\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_uniform',\n",
    "                 input_shape=(112, 112, 1),\n",
    "                 name='Conv1_1')(inp)\n",
    "    hid = Conv2D(32, (3, 3),\n",
    "                 activation='relu',\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_uniform',\n",
    "                 name='Conv1_2')(inp)\n",
    "    hid = MaxPooling2D((2, 2), name='MaxPool1')(hid)\n",
    "    hid = Dropout(0.2)(hid)\n",
    "\n",
    "    hid = Conv2D(64, (3, 3),\n",
    "                 activation=\"relu\",\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_uniform',\n",
    "                 name='Conv2_1')(hid)\n",
    "    hid = Conv2D(64, (3, 3),\n",
    "                 activation=\"relu\",\n",
    "                 padding='same',\n",
    "                 kernel_initializer='he_uniform',\n",
    "                 name='Conv2_2')(hid)\n",
    "    hid = MaxPooling2D((2, 2), name='MaxPool2')(hid)\n",
    "    hid = Dropout(0.2)(hid)\n",
    "\n",
    "    hid = Flatten()(hid)\n",
    "    hid = Dense(64, activation='relu', kernel_initializer='he_uniform')(hid)\n",
    "    hid = Dropout(0.2)(hid)\n",
    "\n",
    "    out = Dense(2, activation='softmax')(hid)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=out)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Base Model\n",
    "\n",
    "The *Base Model* is trained on the set that has been pre-processed with the basic pre-processing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model =  build_model()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training: Importing weights from pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the VGG16 weight file\n",
    "f = h5py.File(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "    'r')\n",
    "\n",
    "# Select the layers for which you want to set weight.\n",
    "\n",
    "w, b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1'][\n",
    "    'block1_conv1_b_1:0']\n",
    "base_model.layers[1].set_weights = [w, b]\n",
    "\n",
    "w, b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2'][\n",
    "    'block1_conv2_b_1:0']\n",
    "base_model.layers[2].set_weights = [w, b]\n",
    "\n",
    "f.close()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training: Training Set\n",
    "\n",
    "The model is trained on the training dataset with a batch size of 64 and for 10 epochs. Early Stopping and Checkpoint are also set, so that the best model is saved.\n",
    "\n",
    "The code below can be skipped since it is a time-consuming one, and the model can be loaded directly from the saved file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_epochs = 10\n",
    "\n",
    "# Compile model\n",
    "es = EarlyStopping(patience=5)\n",
    "chkpt = ModelCheckpoint(\n",
    "    filepath=\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/best_initial_model',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "\n",
    "base_model.compile(loss='binary_crossentropy',\n",
    "                   optimizer=Adam(learning_rate=0.0001, decay=1e-5),\n",
    "                   metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "base_model.fit(train_data,\n",
    "               train_labels,\n",
    "               batch_size=batch_size,\n",
    "               epochs=nb_epochs,\n",
    "               callbacks=[es, chkpt],\n",
    "               validation_data=(valid_data, valid_labels))\n",
    "\n",
    "base_model.save(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/base_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspection of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "base_model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/ImageProcessing/base_model.h5')\n",
    "\n",
    "# Get overall accuracy\n",
    "test_loss, test_accuracy = base_model.evaluate(test_data,\n",
    "                                               test_labels,\n",
    "                                               verbose=0)\n",
    "print('Total Accuracy on Test set=' + '> %.3f' % (test_accuracy * 100.0) + '%')\n",
    "\n",
    "# Get predictions\n",
    "preds = base_model.predict(test_data_4, batch_size=64)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Original labels\n",
    "orig_test_labels = np.argmax(test_labels_4, axis=-1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(orig_test_labels, preds)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, figsize=(12, 8), hide_ticks=True, cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Simple Pre-processing\n",
    "\n",
    "A second model is trained, validated, and tested on data that have been pre-processed with the simple pre-processing steps.\n",
    "\n",
    "##### Model Training: Importing weights from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_model = build_model()\n",
    "\n",
    "# Open the VGG16 weight file\n",
    "f = h5py.File(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "    'r')\n",
    "\n",
    "# Select the layers for which you want to set weight.\n",
    "\n",
    "w, b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1'][\n",
    "    'block1_conv1_b_1:0']\n",
    "second_model.layers[1].set_weights = [w, b]\n",
    "\n",
    "w, b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2'][\n",
    "    'block1_conv2_b_1:0']\n",
    "second_model.layers[2].set_weights = [w, b]\n",
    "\n",
    "f.close()\n",
    "second_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training: Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_epochs = 10\n",
    "\n",
    "# Compile model\n",
    "es = EarlyStopping(patience=5)\n",
    "chkpt = ModelCheckpoint(\n",
    "    filepath=\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/best_second_model',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "\n",
    "second_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer=Adam(learning_rate=0.0001, decay=1e-5),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "second_model.fit(train_data_2,\n",
    "                 train_labels_2,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=nb_epochs,\n",
    "                 callbacks=[es, chkpt],\n",
    "                 validation_data=(valid_data_2, valid_labels_2))\n",
    "\n",
    "second_model.save(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/second_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspection of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "second_model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/ImageProcessing/second_model.h5')\n",
    "\n",
    "# Get overall accuracy\n",
    "test_loss, test_accuracy = base_model.evaluate(test_data_2,\n",
    "                                               test_labels_2,\n",
    "                                               verbose=0)\n",
    "print('Total Accuracy on Test set=' + '> %.3f' % (test_accuracy * 100.0) + '%')\n",
    "\n",
    "# Get predictions\n",
    "preds = base_model.predict(test_data_4, batch_size=64)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Original labels\n",
    "orig_test_labels = np.argmax(test_labels_4, axis=-1)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(orig_test_labels, preds)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm, figsize=(12, 8), hide_ticks=True, cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model with Histogram Normalization Pre-processing\n",
    "\n",
    "Another model is trained, validated, and tested on data that have been pre-processed with the simple pre-processing steps coupled with histogram normalization.\n",
    "\n",
    "##### Model Training: Importing weights from pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fourth_model = build_model()\n",
    "\n",
    "# Open the VGG16 weight file\n",
    "f = h5py.File(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "    'r')\n",
    "\n",
    "# Select the layers for which you want to set weight.\n",
    "\n",
    "w, b = f['block1_conv1']['block1_conv1_W_1:0'], f['block1_conv1'][\n",
    "    'block1_conv1_b_1:0']\n",
    "fourth_model.layers[1].set_weights = [w, b]\n",
    "\n",
    "w, b = f['block1_conv2']['block1_conv2_W_1:0'], f['block1_conv2'][\n",
    "    'block1_conv2_b_1:0']\n",
    "fourth_model.layers[2].set_weights = [w, b]\n",
    "\n",
    "f.close()\n",
    "fourth_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training: Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "nb_epochs = 10\n",
    "\n",
    "# Compile model\n",
    "es = EarlyStopping(patience=5)\n",
    "chkpt = ModelCheckpoint(\n",
    "    filepath=\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/best_fourth_model',\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True)\n",
    "\n",
    "fourth_model.compile(loss='binary_crossentropy',\n",
    "                     optimizer=Adam(learning_rate=0.0001, decay=1e-5),\n",
    "                     metrics=[\"accuracy\"])\n",
    "\n",
    "# Fit the model\n",
    "fourth_model.fit(train_data_4,\n",
    "                 train_labels_4,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=nb_epochs,\n",
    "                 callbacks=[es, chkpt],\n",
    "                 validation_data=(valid_data_4, valid_labels_4))\n",
    "\n",
    "fourth_model.save(\n",
    "    '/content/drive/MyDrive/Colab Notebooks/ImageProcessing/fourth_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inspection of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "fourth_model = keras.models.load_model('/content/drive/MyDrive/Colab Notebooks/ImageProcessing/fourth_model.h5')\n",
    "\n",
    "# Get predictions\n",
    "preds = fourth_model.predict(test_data_4, batch_size=64)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "\n",
    "# Original labels\n",
    "orig_test_labels = np.argmax(test_labels_4, axis=-1)\n",
    "\n",
    "print(orig_test_labels.shape)\n",
    "print(preds.shape)\n",
    "\n",
    "# Get the confusion matrix\n",
    "cm  = confusion_matrix(orig_test_labels, preds)\n",
    "\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, cmap=plt.cm.Blues)\n",
    "plt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
